{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dqn.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOZzx6S84M0APNdmKVo36vu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"OXm1G1Sq0Imt"},"source":["###############################################\n","###############################################\n","###############################################\n","\n","# Generic imports\n","import os\n","import warnings\n","import random\n","\n","# Import tensorflow and filter warning messages\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '10'\n","warnings.filterwarnings('ignore',category=FutureWarning)\n","import tensorflow                    as     tf\n","import tensorflow_probability        as     tfp\n","from   tensorflow.keras              import Model\n","from   tensorflow.keras.layers       import Dense\n","from   tensorflow.keras.optimizers   import Adam\n","\n","# Define alias\n","tfd = tfp.distributions\n","\n","###############################################\n","### Q-network\n","class q_network(Model):\n","    def __init__(self, arch, act_dim, lr):\n","        super(q_network, self).__init__()\n","\n","        # Define network\n","        self.ac = []\n","        for layer in range(len(arch)):\n","            self.ac.append(Dense(arch[layer],\n","                                 activation = 'relu'))\n","        self.ac.append(Dense(act_dim,\n","                             activation = 'linear'))\n","\n","        # Define optimizer\n","        self.opt = Adam(learning_rate = lr,\n","                        clipvalue     = 100.0)\n","\n","    # Network forward pass\n","    def call(self, state):\n","\n","        # Copy inputs\n","        var = state\n","\n","        # Compute output\n","        for layer in range(len(self.ac)):\n","            var = self.ac[layer](var)\n","\n","        return var\n","\n","###############################################\n","### Ring buffer\n","\n","##########\n","# TO DO\n","# Implement a ring buffer class:\n","# - That contains a fixed-size list of size ring_size\n","# - With an \"append\" function to add an element to it\n","# - With a \"get_mini_batch\" function to obtain a shuffled\n","#   mini batch from an input set of indices\n","# - With a \"check\" function that returns the number of elements in it\n","##########\n","class ring_buffer():\n","    # Your buffer must have a fixed lenght, and be a list\n","    # Check how to implement an empty list of given size\n","    # You will need to declare a few other things to make it work\n","    def __init__(self, ring_size):\n","\n","\n","    # Append element\n","    # The input element must be added to the buffer in a circular way\n","    # If the end of the buffer is reached, the new element must overwrite\n","    # the oldest element in the buffer (hence circular)\n","    def append(self, elt):\n","\n","\n","    # Get mini-batch given a list of indices\n","    # Given a list of indices (in the correct range that depends on the\n","    # current filling level of the buffer), return a mini-batch with the\n","    # corresponding elements\n","    def get_mini_batch(self, idx):\n","\n","\n","    # Return global index\n","    # This function must return the filling level of the buffer\n","    # If the buffer has already been filled entirely once, return the global size\n","    # If not, return the number of elements that have already been added\n","    def check(self):\n","\n","\n","###############################################\n","###############################################\n","###############################################\n","\n","# Generic imports\n","import gym\n","import math\n","import numpy as np\n","import copy  as cp\n","\n","###############################################\n","### A DQN agent\n","class dqn_agent:\n","    def __init__(self, act_dim, obs_dim, lr, arch, gamma, mem_size,\n","                 batch_size, target_freq, eps_start, eps_end, n_ep_decay):\n","\n","        # Initialize from arguments\n","        self.act_dim     = act_dim\n","        self.obs_dim     = obs_dim\n","        self.lr          = lr\n","        self.arch        = arch\n","        self.gamma       = gamma\n","        self.mem_size    = mem_size\n","        self.batch_size  = batch_size\n","        self.target_freq = target_freq\n","        self.eps_start   = eps_start\n","        self.eps_end     = eps_end\n","        self.n_ep_decay  = n_ep_decay\n","        self.eps         = eps_start\n","\n","        # Initialize target update counter\n","        self.tgt_update  = 0\n","\n","        # Initialize buffers\n","        self.buff_rwd = ring_buffer(self.mem_size)\n","        self.buff_obs = ring_buffer(self.mem_size)\n","        self.buff_nxt = ring_buffer(self.mem_size)\n","        self.buff_act = ring_buffer(self.mem_size)\n","        self.buff_trm = ring_buffer(self.mem_size)\n","\n","        # Build networks\n","        self.q_net = q_network(self.arch,\n","                               self.act_dim,\n","                               self.lr)\n","        self.q_tgt = q_network(self.arch,\n","                               self.act_dim,\n","                               self.lr)\n","\n","        # Init parameters\n","        dummy = self.q_net(tf.ones([1,self.obs_dim]))\n","        dummy = self.q_tgt(tf.ones([1,self.obs_dim]))\n","\n","    # Get actions from network\n","    def get_actions(self, obs):\n","\n","        # Handle epsilon-greedy strategy\n","        p = random.uniform(0, 1)\n","        if (p < self.eps):\n","            action = random.randrange(0, self.act_dim)\n","        else:\n","            obs    = tf.cast([obs], tf.float32)\n","            values = self.q_net(obs)\n","            values = values.numpy()\n","            action = np.argmax(values)\n","\n","        return action\n","\n","    # Update epsilon with linear decay\n","    def update_epsilon(self, ep):\n","\n","        r        = min(float(ep/self.n_ep_decay),1.0)\n","        self.eps = self.eps_start + r*(self.eps_end-self.eps_start)\n","\n","    # Train network\n","    def train(self):\n","\n","        # Check that ring buffer has enough samples\n","        n = self.buff_rwd.check()\n","        if (n < self.batch_size ): return\n","\n","        # Get values from ring buffers\n","        idx = random.sample(range(0,n),self.batch_size)\n","        rwd = self.buff_rwd.get_mini_batch(idx)\n","        obs = self.buff_obs.get_mini_batch(idx)\n","        nxt = self.buff_nxt.get_mini_batch(idx)\n","        act = self.buff_act.get_mini_batch(idx)\n","        trm = self.buff_trm.get_mini_batch(idx)\n","\n","        # Cast and reshape\n","        rwd = tf.cast(rwd, tf.float32)\n","        obs = tf.cast(obs, tf.float32)\n","        nxt = tf.cast(nxt, tf.float32)\n","        act = tf.cast(act, tf.int32)\n","        trm = tf.cast(trm, tf.float32)\n","\n","        rwd = tf.reshape(rwd, [-1,1])\n","        obs = tf.reshape(obs, [-1,self.obs_dim])\n","        nxt = tf.reshape(nxt, [-1,self.obs_dim])\n","        act = tf.reshape(act, [-1,1])\n","        trm = tf.reshape(trm, [-1,1])\n","\n","        # Train\n","        self.train_dqn(rwd, obs, nxt, act, trm)\n","\n","        # Update target if necessary\n","        if (self.tgt_update == self.target_freq):\n","            self.q_tgt.set_weights(self.q_net.get_weights())\n","            self.tgt_update  = 0\n","        else:\n","            self.tgt_update += 1\n","\n","    # Unroll episode\n","    def unroll_episode(self, env, score, ep, n_ep):\n","\n","        # Reset observation and done flag\n","        obs  = env.reset()\n","        done = False\n","\n","        # Loop\n","        while (not done):\n","\n","            # Make one iteration\n","            act               = self.get_actions(obs)\n","            nxt, rwd, done, _ = env.step(act)\n","\n","            # Store in buffers\n","            self.buff_obs.append(obs)\n","            self.buff_nxt.append(nxt)\n","            self.buff_rwd.append(rwd)\n","            self.buff_act.append(act)\n","            self.buff_trm.append(float(done))\n","\n","            # Update observation\n","            obs = nxt\n","\n","            # Update score\n","            score[ep] += rwd\n","\n","            # Train agent\n","            self.train()\n","\n","        # Print\n","        self.print_episode(ep, n_ep, score)\n","\n","        # Update epsilon\n","        self.update_epsilon(ep)\n","\n","    # Training function for actor\n","    @tf.function\n","    def train_dqn(self, rwd, obs, nxt, act, trm):\n","        with tf.GradientTape() as tape:\n","\n","            # Compute loss\n","            tgt  = tf.reshape(tf.reduce_max(self.q_tgt(nxt),axis=1), [-1,1])\n","            tgt  = rwd + (1.0-trm)*self.gamma*tgt\n","            val  = tf.gather(self.q_net(obs), act, axis=1, batch_dims=1)\n","            diff = tf.square(tgt - val)\n","            loss = tf.reduce_mean(diff)\n","\n","            # Apply gradients\n","            q_var = self.q_net.trainable_variables\n","            grads = tape.gradient(loss, q_var)\n","\n","        self.q_net.opt.apply_gradients(zip(grads,q_var))\n","\n","    # Printings at the end of an episode\n","    def print_episode(self, ep, n_ep, score):\n","\n","        if (ep < 1) or (not (ep%50 == 0)): return\n","        lgt = min(ep, 25)\n","        avg = np.mean(score[ep-lgt:ep])\n","        avg = f\"{avg:.3f}\"\n","\n","        print('# Ep #'+str(ep)+', avg score = '+str(avg), end='\\n')\n","\n","###############################################\n","###############################################\n","###############################################\n","\n","# Generic imports\n","import time\n","\n","# Process training\n","def train(run, env_name, n_ep, lr, arch, gamma, mem_size, batch_size,\n","          update_freq, eps_start, eps_end, n_ep_decay):\n","\n","    # Declare environement and agent\n","    env     = gym.make(env_name)\n","    act_dim = env.action_space.n\n","    obs_dim = env.observation_space.shape[0]\n","    agent   = dqn_agent(act_dim, obs_dim, lr, arch, gamma,\n","                        mem_size, batch_size, update_freq,\n","                        eps_start, eps_end, n_ep_decay)\n","    score   = np.zeros(n_ep)\n","\n","    # Loop until max episode number is reached\n","    for ep in range(n_ep):\n","        agent.unroll_episode(env, score, ep, n_ep)\n","\n","    # Close environments\n","    env.close()\n","\n","    # Return array of episode scores\n","    return score\n","\n","###############################################\n","###############################################\n","###############################################\n","\n","# Generic imports\n","import matplotlib.pyplot as plt\n","\n","# Plot avg/std of score as a function of episodes\n","def plot_score(score, title):\n","\n","    ##########\n","    # TO DO\n","    # - Compute a sliding average of the scores (optional)\n","    # - Compute avg and std of score over n_avg runs\n","    # - Plot avg, avg-std, avg+std as a function of episodes\n","    ##########\n","\n","\n","###############################################\n","###############################################\n","###############################################\n","\n","# Parameters\n","env_name    = \"CartPole-v0\" # Name of the environment\n","n_run       = 2             # Nb of runs for results averaging\n","n_ep        = 1000          # Nb of episodes in each run\n","lr          = 5.0e-4        # Actor learning rate\n","arch        = [32,32]       # Actor architecture\n","gamma       = 0.99          # Discount value\n","mem_size    = 10000         # Size of memory replay\n","batch_size  = 64            # Batch size for training\n","target_freq = 100           # Update frequency for target network\n","eps_start   = 0.1           # Initial epsilon-greedy value\n","eps_end     = 0.05          # Final   epsilon-greedy value\n","n_ep_decay  = 500           # Nb of episodes on which decay happens\n","title       = 'CartPole-v0, dqn'\n","\n","# Perform multiple runs\n","score = np.zeros((n_run, n_ep))\n","for ep in range(n_run):\n","    print('### Avg run #'+str(ep))\n","    start_time   = time.time()\n","    score[ep, :] = train(ep, env_name, n_ep, lr, arch, gamma,\n","                         mem_size, batch_size, target_freq,\n","                         eps_start, eps_end, n_ep_decay)\n","    ctime        = time.time() - start_time\n","    ctime        = f\"{ctime:.3f}\"\n","    print(\"--- \"+ctime+\" seconds ---\")\n","\n","# Plot score\n","plot_score(score, title)"],"execution_count":null,"outputs":[]}]}